{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sys import stderr\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import scipy.misc\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG19_LAYERS = (\n",
    "    'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "\n",
    "    'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "\n",
    "    'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "    'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "\n",
    "    'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "    'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "\n",
    "    'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "    'relu5_3', 'conv5_4', 'relu5_4'\n",
    ")\n",
    "\n",
    "CONTENT_LAYERS = ('relu4_2', 'relu5_2')\n",
    "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')\n",
    "\n",
    "try:\n",
    "    reduce\n",
    "except NameError:\n",
    "    from functools import reduce\n",
    "\n",
    "def load_net(data_path):\n",
    "    data = scipy.io.loadmat(data_path)\n",
    "    if not all(i in data for i in ('layers', 'classes', 'normalization')):\n",
    "        raise ValueError(\"You're using the wrong VGG19 data. Please follow the instructions in the README to download the correct data.\")\n",
    "    mean = data['normalization'][0][0][0]\n",
    "    mean_pixel = np.mean(mean, axis=(0, 1))\n",
    "    weights = data['layers'][0]\n",
    "    return weights, mean_pixel\n",
    "\n",
    "def net_preloaded(weights, input_image, pooling):\n",
    "    net = {}\n",
    "    current = input_image\n",
    "    for i, name in enumerate(VGG19_LAYERS):\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels, bias = weights[i][0][0][0][0]\n",
    "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "            kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
    "            bias = bias.reshape(-1)\n",
    "            current = _conv_layer(current, kernels, bias)\n",
    "        elif kind == 'relu':\n",
    "            current = tf.nn.relu(current)\n",
    "        elif kind == 'pool':\n",
    "            current = _pool_layer(current, pooling)\n",
    "        net[name] = current\n",
    "\n",
    "    assert len(net) == len(VGG19_LAYERS)\n",
    "    return net\n",
    "\n",
    "def _conv_layer(input, weights, bias):\n",
    "    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n",
    "            padding='SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "\n",
    "def _pool_layer(input, pooling):\n",
    "    if pooling == 'avg':\n",
    "        return tf.nn.avg_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                padding='SAME')\n",
    "    else:\n",
    "        return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                padding='SAME')\n",
    "\n",
    "def preprocess(image, mean_pixel):\n",
    "    return image - mean_pixel\n",
    "\n",
    "\n",
    "def unprocess(image, mean_pixel):\n",
    "    return image + mean_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stylize(network, initial, initial_noiseblend, content, styles, preserve_colors, iterations,\n",
    "        content_weight, content_weight_blend, style_weight, style_layer_weight_exp, style_blend_weights, tv_weight,\n",
    "        learning_rate, beta1, beta2, epsilon, pooling,\n",
    "        print_iterations=None, checkpoint_iterations=None):\n",
    "    \"\"\"\n",
    "    Stylize images.\n",
    "    This function yields tuples (iteration, image); `iteration` is None\n",
    "    if this is the final image (the last iteration).  Other tuples are yielded\n",
    "    every `checkpoint_iterations` iterations.\n",
    "    :rtype: iterator[tuple[int|None,image]]\n",
    "    \"\"\"\n",
    "    shape = (1,) + content.shape\n",
    "    style_shapes = [(1,) + style.shape for style in styles]\n",
    "    content_features = {}\n",
    "    style_features = [{} for _ in styles]\n",
    "\n",
    "    vgg_weights, vgg_mean_pixel = load_net(network)\n",
    "\n",
    "    layer_weight = 1.0\n",
    "    style_layers_weights = {}\n",
    "    for style_layer in STYLE_LAYERS:\n",
    "        style_layers_weights[style_layer] = layer_weight\n",
    "        layer_weight *= style_layer_weight_exp\n",
    "\n",
    "    # normalize style layer weights\n",
    "    layer_weights_sum = 0\n",
    "    for style_layer in STYLE_LAYERS:\n",
    "        layer_weights_sum += style_layers_weights[style_layer]\n",
    "    for style_layer in STYLE_LAYERS:\n",
    "        style_layers_weights[style_layer] /= layer_weights_sum\n",
    "\n",
    "    # compute content features in feedforward mode\n",
    "    g = tf.Graph()\n",
    "    with g.as_default(), g.device('/gpu:0'), tf.Session() as sess:\n",
    "        image = tf.placeholder('float', shape=shape)\n",
    "        net = net_preloaded(vgg_weights, image, pooling)\n",
    "        content_pre = np.array([preprocess(content, vgg_mean_pixel)])\n",
    "        for layer in CONTENT_LAYERS:\n",
    "            content_features[layer] = net[layer].eval(feed_dict={image: content_pre})\n",
    "\n",
    "    # compute style features in feedforward mode\n",
    "    for i in range(len(styles)):\n",
    "        g = tf.Graph()\n",
    "        with g.as_default(), g.device('/gpu:0'), tf.Session() as sess:\n",
    "            image = tf.placeholder('float', shape=style_shapes[i])\n",
    "            net = net_preloaded(vgg_weights, image, pooling)\n",
    "            style_pre = np.array([preprocess(styles[i], vgg_mean_pixel)])\n",
    "            for layer in STYLE_LAYERS:\n",
    "                features = net[layer].eval(feed_dict={image: style_pre})\n",
    "                features = np.reshape(features, (-1, features.shape[3]))\n",
    "                gram = np.matmul(features.T, features) / features.size\n",
    "                style_features[i][layer] = gram\n",
    "\n",
    "    initial_content_noise_coeff = 1.0 - initial_noiseblend\n",
    "\n",
    "    # make stylized image using backpropogation\n",
    "    with tf.Graph().as_default():\n",
    "        if initial is None:\n",
    "            noise = np.random.normal(size=shape, scale=np.std(content) * 0.1)\n",
    "            initial = tf.random_normal(shape) * 0.256\n",
    "        else:\n",
    "            initial = np.array([preprocess(initial, vgg_mean_pixel)])\n",
    "            initial = initial.astype('float32')\n",
    "            noise = np.random.normal(size=shape, scale=np.std(content) * 0.1)\n",
    "            initial = (initial) * initial_content_noise_coeff + (tf.random_normal(shape) * 0.256) * (1.0 - initial_content_noise_coeff)\n",
    "        image = tf.Variable(initial)\n",
    "        net = net_preloaded(vgg_weights, image, pooling)\n",
    "\n",
    "        # content loss\n",
    "        content_layers_weights = {}\n",
    "        content_layers_weights['relu4_2'] = content_weight_blend\n",
    "        content_layers_weights['relu5_2'] = 1.0 - content_weight_blend\n",
    "\n",
    "        content_loss = 0\n",
    "        content_losses = []\n",
    "        for content_layer in CONTENT_LAYERS:\n",
    "            content_losses.append(content_layers_weights[content_layer] * content_weight * (2 * tf.nn.l2_loss(\n",
    "                    net[content_layer] - content_features[content_layer]) /\n",
    "                    content_features[content_layer].size))\n",
    "        content_loss += reduce(tf.add, content_losses)\n",
    "\n",
    "        # style loss\n",
    "        style_loss = 0\n",
    "        for i in range(len(styles)):\n",
    "            style_losses = []\n",
    "            for style_layer in STYLE_LAYERS:\n",
    "                layer = net[style_layer]\n",
    "                _, height, width, number = map(lambda i: i.value, layer.get_shape())\n",
    "                size = height * width * number\n",
    "                feats = tf.reshape(layer, (-1, number))\n",
    "                gram = tf.matmul(tf.transpose(feats), feats) / size\n",
    "                style_gram = style_features[i][style_layer]\n",
    "                style_losses.append(style_layers_weights[style_layer] * 2 * tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
    "            style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n",
    "\n",
    "        # total variation denoising\n",
    "        tv_y_size = _tensor_size(image[:,1:,:,:])\n",
    "        tv_x_size = _tensor_size(image[:,:,1:,:])\n",
    "        tv_loss = tv_weight * 2 * (\n",
    "                (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:shape[1]-1,:,:]) /\n",
    "                    tv_y_size) +\n",
    "                (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:shape[2]-1,:]) /\n",
    "                    tv_x_size))\n",
    "        # overall loss\n",
    "        loss = content_loss + style_loss + tv_loss\n",
    "\n",
    "        # optimizer setup\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)\n",
    "\n",
    "        def print_progress():\n",
    "            stderr.write('  content loss: %g\\n' % content_loss.eval())\n",
    "            stderr.write('    style loss: %g\\n' % style_loss.eval())\n",
    "            stderr.write('       tv loss: %g\\n' % tv_loss.eval())\n",
    "            stderr.write('    total loss: %g\\n' % loss.eval())\n",
    "\n",
    "        # optimization\n",
    "        best_loss = float('inf')\n",
    "        best = None\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            stderr.write('Optimization started...\\n')\n",
    "            if (print_iterations and print_iterations != 0):\n",
    "                print_progress()\n",
    "            for i in range(iterations):\n",
    "                stderr.write('Iteration %4d/%4d\\n' % (i + 1, iterations))\n",
    "                train_step.run()\n",
    "\n",
    "                last_step = (i == iterations - 1)\n",
    "                if last_step or (print_iterations and i % print_iterations == 0):\n",
    "                    print_progress()\n",
    "\n",
    "                if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step:\n",
    "                    this_loss = loss.eval()\n",
    "                    if this_loss < best_loss:\n",
    "                        best_loss = this_loss\n",
    "                        best = image.eval()\n",
    "\n",
    "                    img_out = unprocess(best.reshape(shape[1:]), vgg_mean_pixel)\n",
    "\n",
    "                    if preserve_colors and preserve_colors == True:\n",
    "                        original_image = np.clip(content, 0, 255)\n",
    "                        styled_image = np.clip(img_out, 0, 255)\n",
    "\n",
    "                        # Luminosity transfer steps:\n",
    "                        # 1. Convert stylized RGB->grayscale accoriding to Rec.601 luma (0.299, 0.587, 0.114)\n",
    "                        # 2. Convert stylized grayscale into YUV (YCbCr)\n",
    "                        # 3. Convert original image into YUV (YCbCr)\n",
    "                        # 4. Recombine (stylizedYUV.Y, originalYUV.U, originalYUV.V)\n",
    "                        # 5. Convert recombined image from YUV back to RGB\n",
    "\n",
    "                        # 1\n",
    "                        styled_grayscale = rgb2gray(styled_image)\n",
    "                        styled_grayscale_rgb = gray2rgb(styled_grayscale)\n",
    "\n",
    "                        # 2\n",
    "                        styled_grayscale_yuv = np.array(Image.fromarray(styled_grayscale_rgb.astype(np.uint8)).convert('YCbCr'))\n",
    "\n",
    "                        # 3\n",
    "                        original_yuv = np.array(Image.fromarray(original_image.astype(np.uint8)).convert('YCbCr'))\n",
    "\n",
    "                        # 4\n",
    "                        w, h, _ = original_image.shape\n",
    "                        combined_yuv = np.empty((w, h, 3), dtype=np.uint8)\n",
    "                        combined_yuv[..., 0] = styled_grayscale_yuv[..., 0]\n",
    "                        combined_yuv[..., 1] = original_yuv[..., 1]\n",
    "                        combined_yuv[..., 2] = original_yuv[..., 2]\n",
    "\n",
    "                        # 5\n",
    "                        img_out = np.array(Image.fromarray(combined_yuv, 'YCbCr').convert('RGB'))\n",
    "\n",
    "\n",
    "                    yield (\n",
    "                        (None if last_step else i),\n",
    "                        img_out\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _tensor_size(tensor):\n",
    "    from operator import mul\n",
    "    return reduce(mul, (d.value for d in tensor.get_shape()), 1)\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def gray2rgb(gray):\n",
    "    w, h = gray.shape\n",
    "    rgb = np.empty((w, h, 3), dtype=np.float32)\n",
    "    rgb[:, :, 2] = rgb[:, :, 1] = rgb[:, :, 0] = gray\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imgread(path):\n",
    "    img = imread(path).astype(np.float)\n",
    "    if len(img.shape) == 2:\n",
    "        # grayscale\n",
    "        img = np.dstack((img,img,img))\n",
    "    elif img.shape[2] == 4:\n",
    "        # PNG with alpha channel\n",
    "        img = img[:,:,:3]\n",
    "    return img\n",
    "\n",
    "\n",
    "def imgsave(path, img):\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(img).save(path, quality=95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_image = scipy.misc.imread('content.jpg')\n",
    "style_images = [scipy.misc.imread(style) for style in ['style.jpg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization started...\n",
      "  content loss: 3.99966e+06\n",
      "    style loss: 1.45465e+07\n",
      "       tv loss: 26.1128\n",
      "    total loss: 1.85462e+07\n",
      "Iteration    1/ 100\n",
      "  content loss: 3.66632e+06\n",
      "    style loss: 1.3471e+07\n",
      "       tv loss: 22446.2\n",
      "    total loss: 1.71597e+07\n",
      "Iteration    2/ 100\n",
      "  content loss: 3.31476e+06\n",
      "    style loss: 1.27228e+07\n",
      "       tv loss: 50866.9\n",
      "    total loss: 1.60885e+07\n",
      "Iteration    3/ 100\n",
      "  content loss: 3.22153e+06\n",
      "    style loss: 1.07911e+07\n",
      "       tv loss: 64987.1\n",
      "    total loss: 1.40776e+07\n",
      "Iteration    4/ 100\n",
      "  content loss: 3.16304e+06\n",
      "    style loss: 9.0643e+06\n",
      "       tv loss: 73905.2\n",
      "    total loss: 1.23012e+07\n",
      "Iteration    5/ 100\n",
      "  content loss: 3.12825e+06\n",
      "    style loss: 7.89368e+06\n",
      "       tv loss: 82361.6\n",
      "    total loss: 1.11043e+07\n",
      "Iteration    6/ 100\n",
      "  content loss: 3.11477e+06\n",
      "    style loss: 6.66804e+06\n",
      "       tv loss: 89150.3\n",
      "    total loss: 9.87196e+06\n",
      "Iteration    7/ 100\n",
      "  content loss: 3.10741e+06\n",
      "    style loss: 5.7629e+06\n",
      "       tv loss: 94832.8\n",
      "    total loss: 8.96514e+06\n",
      "Iteration    8/ 100\n",
      "  content loss: 3.09122e+06\n",
      "    style loss: 4.962e+06\n",
      "       tv loss: 99303.6\n",
      "    total loss: 8.15252e+06\n",
      "Iteration    9/ 100\n",
      "  content loss: 3.07813e+06\n",
      "    style loss: 4.36761e+06\n",
      "       tv loss: 102535\n",
      "    total loss: 7.54828e+06\n",
      "Iteration   10/ 100\n",
      "  content loss: 3.06655e+06\n",
      "    style loss: 3.8274e+06\n",
      "       tv loss: 104589\n",
      "    total loss: 6.99854e+06\n",
      "Iteration   11/ 100\n",
      "  content loss: 3.0482e+06\n",
      "    style loss: 3.38642e+06\n",
      "       tv loss: 105803\n",
      "    total loss: 6.54043e+06\n",
      "Iteration   12/ 100\n",
      "  content loss: 3.01955e+06\n",
      "    style loss: 3.03124e+06\n",
      "       tv loss: 106366\n",
      "    total loss: 6.15716e+06\n",
      "Iteration   13/ 100\n",
      "  content loss: 2.98247e+06\n",
      "    style loss: 2.76178e+06\n",
      "       tv loss: 106396\n",
      "    total loss: 5.85066e+06\n",
      "Iteration   14/ 100\n",
      "  content loss: 2.93902e+06\n",
      "    style loss: 2.50565e+06\n",
      "       tv loss: 106066\n",
      "    total loss: 5.55074e+06\n",
      "Iteration   15/ 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1f6c7c42f5ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mpooling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mcheckpoint_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m ):\n\u001b[0;32m     31\u001b[0m     \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e80e0d9f35e4>\u001b[0m in \u001b[0;36mstylize\u001b[1;34m(network, initial, initial_noiseblend, content, styles, preserve_colors, iterations, content_weight, content_weight_blend, style_weight, style_layer_weight_exp, style_blend_weights, tv_weight, learning_rate, beta1, beta2, epsilon, pooling, print_iterations, checkpoint_iterations)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Iteration %4d/%4d\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                 \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mlast_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \"\"\"\n\u001b[1;32m-> 1706\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3961\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3962\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3963\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_shape = content_image.shape\n",
    "for i in range(len(style_images)):\n",
    "    style_scale = 1.0\n",
    "    style_images[i] = scipy.misc.imresize(style_images[i], style_scale * target_shape[1] / style_images[i].shape[1])\n",
    "\n",
    "style_blend_weights = [1.0/len(style_images) for _ in style_images]\n",
    "initial = content_image\n",
    "\n",
    "for iteration, image in stylize(\n",
    "    network='imagenet-vgg-verydeep-19.mat',\n",
    "    initial=initial,\n",
    "    initial_noiseblend=1,\n",
    "    content=content_image,\n",
    "    styles=style_images,\n",
    "    preserve_colors=None,\n",
    "    iterations=100,\n",
    "    content_weight=5e0,\n",
    "    content_weight_blend=1,\n",
    "    style_weight=5e2,\n",
    "    style_layer_weight_exp=1,\n",
    "    style_blend_weights=[1.0/len(style_images) for _ in style_images],\n",
    "    tv_weight=1e2,\n",
    "    learning_rate=1e1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    pooling='max',\n",
    "    print_iterations=True,\n",
    "    checkpoint_iterations=None\n",
    "):\n",
    "    scipy.misc.imsave('output.jpg', image)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
